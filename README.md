
# ğŸ¤– RAG Chatbot â€“ Ask Questions Based on PDF and Word Documents

This project is a **RAG (Retrieval-Augmented Generation)** chatbot that answers your questions based on uploaded **PDF or Word documents**. It uses a local open-source LLM, performs chunk-based document retrieval, and runs efficiently on limited hardware (16GB GPU). It shows **exact citations** from the source documents to prevent hallucinations.

---

## âœ… Setup Instructions

### 1. Clone this repository
```bash
git clone https://github.com/YOUR_USERNAME/RAG-CHATBOT.git
cd RAG-CHATBOT
```

### 2. Create a virtual environment
```bash
python -m venv venv
venv\Scripts\activate     # On Windows
# OR
source venv/bin/activate  # On Mac/Linux
```

### 3. Install all required packages
```bash
pip install -r requirements.txt
```

### 4. Download and place the model
- Download a GGUF quantized model like `mistral-7b-instruct-v0.2.Q4_K_M.gguf`
- Put it inside the `/models` folder

---

## ğŸš€ How to Use

1. Run the app:
```bash
streamlit run app.py
```

2. Upload `.pdf` or `.docx` documents
3. Ask questions related to the documents
4. The chatbot will respond with an **answer** and **exact source references**

---

## ğŸ—ï¸ Architectural Decisions

- **Frontend**: Streamlit â€“ simple and lightweight UI
- **LLM**: `mistral-7b-instruct` (quantized GGUF format run locally using llama-cpp)
- **Embedding Model**: `all-MiniLM-L6-v2` (384-dim, fast and accurate)
- **Vector DB**: Qdrant â€“ fast local vector search with persistent storage
- **File Parser**: `PyMuPDF` for PDFs, `python-docx` for Word files
- No external API, no LangChain â€“ everything runs locally

---

## ğŸ” Retrieval Approach

- Each document is split into **small chunks** (300â€“500 characters with 50-character overlap)
- Chunks are embedded using `sentence-transformers` MiniLM model
- Stored in **Qdrant** using cosine similarity
- On a question, top relevant chunks are retrieved (`top_k = 3`)
- Prompt is constructed using those chunks and fed to the LLM
- The final answer includes references to the source chunks/pages

---

## ğŸ§© Chunking Strategy

- **Chunk Size**: 300â€“500 characters
- **Overlap**: 50 characters
- This ensures no context is lost and sentences are not cut awkwardly
- Keeps memory usage low while preserving semantic meaning
- Optimized for retrieval accuracy and speed

---

## ğŸ‘€ Observations

- Works well for clearly structured documents (e.g., reports, policies, manuals)
- Citation helps reduce hallucinations
- Handles both `.pdf` and `.docx` files
- Local LLM (Mistral) is powerful and runs well when quantized
- Retrieval can fail if document is very short or poorly structured
- Performance remains stable even on limited hardware

---

## ğŸ–¥ï¸ Hardware Usage

- âœ… Designed to run on:
  - **GPU**: NVIDIA Tesla T4 (16 GB VRAM)
  - **RAM**: 16â€“32 GB
- **Model quantization** allows `Mistral 7B` to run within ~14 GB
- Embedding model (`MiniLM`) is lightweight and CPU-friendly
- Efficient enough for hackathons, local testing, and student projects

---

## ğŸ“‚ Folder Structure

```
rag_chatbot_project/
â”‚
â”œâ”€â”€ app.py                  # Streamlit frontend
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ models/                 # GGUF model files
â”œâ”€â”€ vector_store/           # Qdrant local database
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ chunking.py
â”‚   â”œâ”€â”€ embedding.py
â”‚   â”œâ”€â”€ pdf_docx_loader.py
â”‚   â””â”€â”€ retrieval.py
```

---

## ğŸ“˜ Example

Uploaded Document: `health_policy.pdf`

**Question:** What is the maximum coverage mentioned?

**Answer:** The maximum coverage mentioned is â‚¹5 lakhs. (Source: Page 4)

---

## ğŸ“„ License

MIT License â€“ free for personal, academic, and non-commercial use.

---

**âœ… Built with love for offline, fast, and accurate document-based question answering.**
